{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import  LatentDirichletAllocation\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gobidasu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gobidasu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "this = sys.modules[__name__]\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "this.tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
    "this.lemmatizer = WordNetLemmatizer()\n",
    "this.vocabulary = []\n",
    "this.categories = []\n",
    "this.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def generate_categories():\n",
    "    this.categories = []\n",
    "    \"\"\"Generate the list of categories.\"\"\"\n",
    "    topics = 'all-topics-strings.lc.txt'\n",
    "\n",
    "    with open('./reuters21578/' + topics, 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            this.categories.append(category.strip().lower())\n",
    "\n",
    "\n",
    "def vectorize_docs(documents, w2v_model):\n",
    "    \"\"\"A weird oneshot representation for word2vec.\"\"\"\n",
    "    document_max_num_words = 100\n",
    "    num_features = 500\n",
    "\n",
    "    x = np.zeros(shape=(this.number_of_documents, document_max_num_words,\n",
    "                        num_features)).astype(np.float32)\n",
    "\n",
    "    empty_word = np.zeros(num_features).astype(np.float32)\n",
    "\n",
    "    for idx, document in enumerate(documents):\n",
    "        for jdx, word in enumerate(document):\n",
    "            if jdx == document_max_num_words:\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                if word in w2v_model:\n",
    "                    x[idx, jdx, :] = w2v_model[word]\n",
    "                else:\n",
    "                    x[idx, jdx, :] = empty_word\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def vectorize_categories(categories):\n",
    "    num_categories = len(this.categories)\n",
    "\n",
    "    y = np.zeros(shape=(this.number_of_documents, num_categories)).astype(np.float32)\n",
    "\n",
    "    for idx, key in enumerate(categories.keys()):\n",
    "        y[idx, :] = categories[key]\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def unescape(text):\n",
    "    \"\"\"Unescape charactes.\"\"\"\n",
    "    return saxutils.unescape(text)\n",
    "\n",
    "\n",
    "def unique(arr):\n",
    "    return list(set(arr))\n",
    "\n",
    "\n",
    "def add_to_vocab(elements):\n",
    "    for element in elements:\n",
    "        if element not in this.vocabulary:\n",
    "            this.vocabulary.append(element)\n",
    "\n",
    "\n",
    "def add_to_categories(elements):\n",
    "    for element in elements:\n",
    "        if element not in this.categories:\n",
    "            this.categories.append(element)\n",
    "\n",
    "\n",
    "def transform_to_indices(elements):\n",
    "    res = []\n",
    "    for element in elements:\n",
    "        res.append(this.vocabulary.index(element))\n",
    "    return res\n",
    "\n",
    "\n",
    "def transform_to_category_indices(element):\n",
    "    return this.categories.index(element)\n",
    "\n",
    "\n",
    "def strip_tags(text):\n",
    "    \"\"\"String tags for a better vocabulary.\"\"\"\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "\n",
    "def to_category_onehot(categories):\n",
    "    \"\"\"Create onehot vectors for categories.\"\"\"\n",
    "    target_categories = this.categories\n",
    "    vector = np.zeros(len(target_categories)).astype(np.float32)\n",
    "\n",
    "    for i in range(len(target_categories)):\n",
    "        if target_categories[i] in categories:\n",
    "            vector[i] = 1.0\n",
    "\n",
    "    return vector\n",
    "\n",
    "def read_retuters_files(path=\"./reuters21578/\"):\n",
    "    x_train = {}\n",
    "    x_test = {}\n",
    "    y_train = {}\n",
    "    y_test = {}\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".sgm\"):\n",
    "            print(\"reading \", path + file)\n",
    "            f = open(path + file, 'r')\n",
    "            data = f.read()\n",
    "\n",
    "            soup = BeautifulSoup(data)\n",
    "            posts = soup.findAll(\"reuters\")\n",
    "            \n",
    "            \n",
    "            for post in posts:\n",
    "                post_id = post['newid']\n",
    "                body = unescape(strip_tags(str(post('text')))\n",
    "                                .replace('reuter\\n&#3;', ''))\n",
    "                post_categories = []\n",
    "\n",
    "                topics = post.topics.contents\n",
    "\n",
    "                for topic in topics:\n",
    "                    post_categories.append(strip_tags(str(topic)))\n",
    "\n",
    "                category_onehot = to_category_onehot(post_categories)\n",
    "                \n",
    "                cross_validation_type = post[\"lewissplit\"]\n",
    "                if (cross_validation_type == \"TRAIN\"):\n",
    "                    x_train[post_id] = body\n",
    "                    y_train[post_id] = category_onehot\n",
    "                else:\n",
    "                    x_test[post_id] = body\n",
    "                    y_test[post_id] = category_onehot\n",
    "            \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def tokenize(document):\n",
    "    words = []\n",
    "\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [this.lemmatizer.lemmatize(t.lower()) for t in this.tokenizer.tokenize(sentence)\n",
    "                  if t.lower() not in this.stop_words]\n",
    "        words += tokens\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def tokenize_docs(document):\n",
    "    tokenized_docs = []\n",
    "    this.number_of_documents = len(document)\n",
    "\n",
    "    for key in document.keys():\n",
    "        tokenized_docs.append(tokenize(document[key]))\n",
    "\n",
    "    return tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('reading ', './reuters21578/reut2-004.sgm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /usr/local/Cellar/python/2.7.14/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('reading ', './reuters21578/reut2-010.sgm')\n",
      "('reading ', './reuters21578/reut2-011.sgm')\n",
      "('reading ', './reuters21578/reut2-005.sgm')\n",
      "('reading ', './reuters21578/reut2-013.sgm')\n",
      "('reading ', './reuters21578/reut2-007.sgm')\n",
      "('reading ', './reuters21578/reut2-006.sgm')\n",
      "('reading ', './reuters21578/reut2-012.sgm')\n",
      "('reading ', './reuters21578/reut2-016.sgm')\n",
      "('reading ', './reuters21578/reut2-002.sgm')\n",
      "('reading ', './reuters21578/reut2-003.sgm')\n",
      "('reading ', './reuters21578/reut2-017.sgm')\n",
      "('reading ', './reuters21578/reut2-001.sgm')\n",
      "('reading ', './reuters21578/reut2-015.sgm')\n",
      "('reading ', './reuters21578/reut2-014.sgm')\n",
      "('reading ', './reuters21578/reut2-000.sgm')\n",
      "('reading ', './reuters21578/reut2-019.sgm')\n",
      "('reading ', './reuters21578/reut2-018.sgm')\n",
      "('reading ', './reuters21578/reut2-020.sgm')\n",
      "('reading ', './reuters21578/reut2-008.sgm')\n",
      "('reading ', './reuters21578/reut2-009.sgm')\n",
      "('reading ', './reuters21578/reut2-021.sgm')\n"
     ]
    }
   ],
   "source": [
    "generate_categories()\n",
    "(x_train, y_train), (x_test, y_test) = read_retuters_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gobidasu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    " # wordnet\n",
    "num_features = 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:39: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:40: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "x_train_token = tokenize_docs(x_train)\n",
    "\n",
    "w2v_model = Word2Vec(x_train_token,\n",
    "                     size=num_features,\n",
    "                     min_count=1,\n",
    "                     window=10)\n",
    "w2v_model.init_sims(replace=True)\n",
    "\n",
    "x_train = vectorize_docs(x_train_token, w2v_model)\n",
    "y_train = vectorize_categories(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:39: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:40: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "x_test_token = tokenize_docs(x_test)\n",
    "\n",
    "w2v_model = Word2Vec(x_test_token,\n",
    "                         size=num_features,\n",
    "                         min_count=1,\n",
    "                         window=10)\n",
    "\n",
    "w2v_model.init_sims(replace=True)\n",
    "x_test = vectorize_docs(x_test_token, w2v_model)\n",
    "y_test = vectorize_categories(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(14668, 100, 500)\n",
      "(14668, 135)\n",
      "(6910, 100, 500)\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, [14668, 50000])\n",
    "x_test = np.reshape(x_test, [6910, 50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14668, 50000)\n",
      "(14668, 135)\n",
      "(6910, 50000)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, training cost = 353.79593\n",
      "Iteration: 1000, training cost = 95.28506\n",
      "Iteration: 2000, training cost = 89.67037\n",
      "Iteration: 3000, training cost = 70.17113\n",
      "Iteration: 4000, training cost = 34.530533\n",
      "Iteration: 5000, training cost = 29.625038\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 50000])\n",
    "Y = tf.placeholder(tf.float32, [None, 135 ])\n",
    "\n",
    "# Define parameters W and b of your model\n",
    "W1 = tf.get_variable(\"W11\", shape=[50000, 50], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.zeros([50]))\n",
    "W2 = tf.get_variable(\"W12\", shape=[50, 135], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.zeros([135]))\n",
    "\n",
    "# Define your model's tensorflow graph\n",
    "Z1 = tf.matmul(X,W1) + b1\n",
    "A1 = tf.nn.relu(Z1)\n",
    "Z2 = tf.matmul(A1,W2) + b2\n",
    "A2 = tf.nn.softmax(Z2)\n",
    "\n",
    "# Compute the cost function\n",
    "cross_entropy_cost = -tf.reduce_sum(Y * tf.log(A2))\n",
    "\n",
    "# Define accuracy metric\n",
    "num_correct = tf.equal(tf.argmax(A2,1), tf.argmax(Y,1))\n",
    "num_correct = tf.cast(num_correct, tf.float32)\n",
    "accuracy = tf.reduce_mean(num_correct)\n",
    "\n",
    "# Define optimization method, learning rate and the the training step\n",
    "optimizer = tf.train.AdamOptimizer(0.00005)\n",
    "train_step = optimizer.minimize(cross_entropy_cost)\n",
    "\n",
    "# Initialize the variables of the graph, create tensorflow session and run the initialization of global variables.\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Implement the Optimization Loop for 100 iterations\n",
    "for i in range(20000):\n",
    "    # Load batch of images and labels\n",
    "    batch_X, batch_Y = next_batch(100, x_train, y_train)\n",
    "    # Create feed dictionary\n",
    "    feed_dict={X: batch_X, Y: batch_Y}\n",
    "    # Run the session train\n",
    "    _, cost = sess.run([train_step, cross_entropy_cost], feed_dict=feed_dict)\n",
    "    # Print cost and iteration\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Iteration: \" + str(i) + \", training cost = \" + str(cost))\n",
    "\n",
    "# Evaluate your accuracy and cost on the train and test sets\n",
    "train_data={X: x_train, Y: y_train}\n",
    "a,c = sess.run([accuracy, cross_entropy_cost], feed_dict=train_data)\n",
    "print(\"Train accuracy = \" + str(a) + \", Train cost = \" + str(c))\n",
    "\n",
    "test_data={X: x_test, Y: y_test}\n",
    "a,c = sess.run([accuracy, cross_entropy_cost], feed_dict=test_data)\n",
    "print(\"Test accuracy = \" + str(a) + \", Test cost = \" + str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "m = 14668\n",
    "seed = 1 \n",
    "minibatch_size = 64\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.005).minimize(cross_entropy_cost)\n",
    "costs = []   \n",
    "\n",
    "# Initialize all the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start the session to compute the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    # Run the initialization\n",
    "    sess.run(init)\n",
    "        \n",
    "    # Do the training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "        # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "            _ , minibatch_cost = sess.run([optimizer, cross_entropy_cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "            epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "        # Define accuracy metric\n",
    "    num_correct = tf.equal(tf.argmax(A2,1), tf.argmax(Y,1))\n",
    "    num_correct = tf.cast(num_correct, tf.float32)\n",
    "    accuracy = tf.reduce_mean(num_correct)\n",
    "\n",
    "    print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "    print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM  code \n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "import sys\n",
    "\n",
    "this = sys.modules[__name__]\n",
    "\n",
    "\n",
    "def lstm(X_train, Y_train, X_test, Y_test):\n",
    "    \"\"\"Create the LSTM model.\"\"\"\n",
    "    document_max_num_words = 100\n",
    "    num_features = 500\n",
    "    num_categories = 135\n",
    "\n",
    "    tb_callback = keras.callbacks.TensorBoard(log_dir='./tb', histogram_freq=0,\n",
    "                                              write_graph=True, write_images=True)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(int(document_max_num_words * 1.5), input_shape=(document_max_num_words, num_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_categories))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, Y_train, batch_size=128, nb_epoch=5,\n",
    "              validation_data=(X_test, Y_test), callbacks=[tb_callback])\n",
    "\n",
    "    model.save('lstm_reuters.h5')\n",
    "\n",
    "    score, acc = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "\n",
    "    print('Score: %1.4f' % score)\n",
    "    print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    #print('X.shape:', X.shape, 'Y.shape:', Y.shape)\n",
    "    shuffled_X = X[permutation, :, :]\n",
    "    \n",
    "    shuffled_Y = Y[permutation, :] #.reshape((m, Y.shape[1]))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = int(math.floor(m/mini_batch_size)) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "L = l # 1014 \n",
    "NUM_FILTERS = 256 # 1024 for large, 256 for small\n",
    "NOUT = 1024 # 2048 for large, 1024 for small \n",
    "NUM_CHAR_OPTIONS = 70\n",
    "STD_INIT = 0.02 # 0.02 for large, 0.05 for small \n",
    "x = tf.placeholder(tf.float32, [None, L, NUM_CHAR_OPTIONS])\n",
    "y = tf.placeholder(tf.int64, [None, 1])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "num_epochs = 5\n",
    "minibatch_size = 10\n",
    "seed = 1\n",
    "num_categories = 10\n",
    "costs = [] \n",
    "\n",
    "def simple_model(x,y):    \n",
    "    conv1 = tf.layers.conv1d(x, filters=NUM_FILTERS, kernel_size=7, strides=1, padding='SAME', \n",
    "                                   kernel_initializer=tf.initializers.random_normal(mean=0, stddev=STD_INIT), \n",
    "                                   # defaults to NWC\n",
    "                                   activation=tf.nn.relu, reuse=None, name=\"conv1\") \n",
    "    pool1 = tf.layers.max_pooling1d(conv1, pool_size=3, strides=1, padding='VALID', data_format='channels_last', name=\"pool1\") \n",
    "    \n",
    "    conv2 = tf.layers.conv1d(pool1, filters=NUM_FILTERS, kernel_size=7, strides=1, padding='SAME', \n",
    "                                   activation=tf.nn.relu, reuse=None, name=\"conv2\") \n",
    "    pool2 = tf.layers.max_pooling1d(conv2, pool_size=3, strides=1, padding='VALID', data_format='channels_last', name=\"pool2\")\n",
    "    \n",
    "    conv3 = tf.layers.conv1d(pool2, filters=NUM_FILTERS, kernel_size=3, strides=1, padding='SAME', \n",
    "                                   activation=tf.nn.relu, reuse=None, name=\"conv3\") \n",
    "    \n",
    "    conv4 = tf.layers.conv1d(conv3, filters=NUM_FILTERS, kernel_size=3, strides=1, padding='SAME', \n",
    "                                   activation=tf.nn.relu, reuse=None, name=\"conv4\") \n",
    "    \n",
    "    conv5 = tf.layers.conv1d(conv4, filters=NUM_FILTERS, kernel_size=3, strides=1, padding='SAME', \n",
    "                                   activation=tf.nn.relu, reuse=None, name=\"conv5\") \n",
    "    \n",
    "    conv6 = tf.layers.conv1d(conv5, filters=NUM_FILTERS, kernel_size=3, strides=1, padding='SAME', \n",
    "                                   activation=tf.nn.relu, reuse=None, name=\"conv6\") \n",
    "    pool6 = tf.layers.max_pooling1d(conv6, pool_size=3, strides=1, padding='VALID', data_format='channels_last', name=\"pool6\")\n",
    "    \n",
    "    fc7 = tf.contrib.layers.fully_connected(inputs=tf.contrib.layers.flatten(pool6), num_outputs=NOUT, reuse=None, \n",
    "                                      scope=\"fc7\")\n",
    "    fc8 = tf.contrib.layers.fully_connected(inputs=tf.contrib.layers.flatten(fc7), num_outputs=NOUT, reuse=None, \n",
    "                                      scope=\"fc8\")\n",
    "    fc9 = tf.contrib.layers.fully_connected(inputs=tf.contrib.layers.flatten(fc7), num_outputs=num_categories, reuse=None, \n",
    "                                      scope=\"fc9\")\n",
    "    \n",
    "    return fc9\n",
    "\n",
    "print('x.shape', x.shape)\n",
    "print('y.shape', y.shape)\n",
    "print('X_int[0, :,  :]', X_int[0, :,  :].shape)\n",
    "y_out = simple_model(x,y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.nn.softmax_cross_entropy_with_logits(logits = y_out, labels = tf.one_hot(y, 10)) # yahoo \n",
    "\n",
    "# tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(5e-4).minimize(mean_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "              \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # should loop over minibatches\n",
    "    for epoch in range(num_epochs):\n",
    "        #print epoch\n",
    "        epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "        num_minibatches = int(X_int.shape[0] / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X_int, Y_train, minibatch_size, seed)\n",
    "        #minibatches = copy.deepcopy(minibatches_result)\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "            # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "            # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "            ### START CODE HERE ### (1 line)\n",
    "            #print('minibatch shapes:', minibatch_X.shape, minibatch_Y.shape)\n",
    "            _ , minibatch_cost = sess.run([optimizer, mean_loss], feed_dict={x: minibatch_X, y: minibatch_Y})\n",
    "            ### END CODE HERE ###\n",
    "                \n",
    "            epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "        # Print the cost every epoch\n",
    "        if epoch % 1 == 0: #100\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "        if epoch % 1 == 0: #5\n",
    "            costs.append(epoch_cost)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(5e-4))\n",
    "    plt.show()\n",
    "\n",
    "    # lets save the parameters in a variable\n",
    "    #parameters = sess.run(parameters)\n",
    "    #print (\"Parameters have been trained!\")\n",
    "\n",
    "    # Calculate the correct predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(y_out), tf.argmax(y))\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    print (\"Train Accuracy:\", accuracy.eval({X: X_int, Y: Y_train}))\n",
    "    #print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "\n",
    "    #_ , minibatch_cost = sess.run([optimizer, cost], feed_dict={x: X_int, y: Y_train}) # minibatch_X, minibatch_Y\n",
    "    #print (\"Cost:\", minibatch_cost)\n",
    "    #result = sess.run(y_out)\n",
    "    #print result\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
