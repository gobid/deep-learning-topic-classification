{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import  LatentDirichletAllocation\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rahulmakhijani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rahulmakhijani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "this = sys.modules[__name__]\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "this.tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
    "this.lemmatizer = WordNetLemmatizer()\n",
    "this.vocabulary = []\n",
    "this.categories = []\n",
    "this.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def generate_categories():\n",
    "    this.categories = []\n",
    "    \"\"\"Generate the list of categories.\"\"\"\n",
    "    topics = 'all-topics-strings.lc.txt'\n",
    "\n",
    "    with open('./reuters21578/' + topics, 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            this.categories.append(category.strip().lower())\n",
    "\n",
    "\n",
    "def vectorize_docs(documents, w2v_model):\n",
    "    \"\"\"A weird oneshot representation for word2vec.\"\"\"\n",
    "    document_max_num_words = 100\n",
    "    num_features = 500\n",
    "\n",
    "    x = np.zeros(shape=(this.number_of_documents, document_max_num_words,\n",
    "                        num_features)).astype(np.float32)\n",
    "\n",
    "    empty_word = np.zeros(num_features).astype(np.float32)\n",
    "\n",
    "    for idx, document in enumerate(documents):\n",
    "        for jdx, word in enumerate(document):\n",
    "            if jdx == document_max_num_words:\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                if word in w2v_model:\n",
    "                    x[idx, jdx, :] = w2v_model[word]\n",
    "                else:\n",
    "                    x[idx, jdx, :] = empty_word\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def vectorize_categories(categories):\n",
    "    num_categories = len(this.categories)\n",
    "\n",
    "    y = np.zeros(shape=(this.number_of_documents, num_categories)).astype(np.float32)\n",
    "\n",
    "    for idx, key in enumerate(categories.keys()):\n",
    "        y[idx, :] = categories[key]\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def unescape(text):\n",
    "    \"\"\"Unescape charactes.\"\"\"\n",
    "    return saxutils.unescape(text)\n",
    "\n",
    "\n",
    "def unique(arr):\n",
    "    return list(set(arr))\n",
    "\n",
    "\n",
    "def add_to_vocab(elements):\n",
    "    for element in elements:\n",
    "        if element not in this.vocabulary:\n",
    "            this.vocabulary.append(element)\n",
    "\n",
    "\n",
    "def add_to_categories(elements):\n",
    "    for element in elements:\n",
    "        if element not in this.categories:\n",
    "            this.categories.append(element)\n",
    "\n",
    "\n",
    "def transform_to_indices(elements):\n",
    "    res = []\n",
    "    for element in elements:\n",
    "        res.append(this.vocabulary.index(element))\n",
    "    return res\n",
    "\n",
    "\n",
    "def transform_to_category_indices(element):\n",
    "    return this.categories.index(element)\n",
    "\n",
    "\n",
    "def strip_tags(text):\n",
    "    \"\"\"String tags for a better vocabulary.\"\"\"\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "\n",
    "def to_category_onehot(categories):\n",
    "    \"\"\"Create onehot vectors for categories.\"\"\"\n",
    "    target_categories = this.categories\n",
    "    vector = np.zeros(len(target_categories)).astype(np.float32)\n",
    "\n",
    "    for i in range(len(target_categories)):\n",
    "        if target_categories[i] in categories:\n",
    "            vector[i] = 1.0\n",
    "\n",
    "    return vector\n",
    "\n",
    "def read_retuters_files(path=\"./reuters21578/\"):\n",
    "    x_train = {}\n",
    "    x_test = {}\n",
    "    y_train = {}\n",
    "    y_test = {}\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".sgm\"):\n",
    "            print(\"reading \", path + file)\n",
    "            f = open(path + file, 'r')\n",
    "            data = f.read()\n",
    "\n",
    "            soup = BeautifulSoup(data)\n",
    "            posts = soup.findAll(\"reuters\")\n",
    "            \n",
    "            \n",
    "            for post in posts:\n",
    "                post_id = post['newid']\n",
    "                body = unescape(strip_tags(str(post('text')))\n",
    "                                .replace('reuter\\n&#3;', ''))\n",
    "                post_categories = []\n",
    "\n",
    "                topics = post.topics.contents\n",
    "\n",
    "                for topic in topics:\n",
    "                    post_categories.append(strip_tags(str(topic)))\n",
    "\n",
    "                category_onehot = to_category_onehot(post_categories)\n",
    "                \n",
    "                cross_validation_type = post[\"lewissplit\"]\n",
    "                if (cross_validation_type == \"TRAIN\"):\n",
    "                    x_train[post_id] = body\n",
    "                    y_train[post_id] = category_onehot\n",
    "                else:\n",
    "                    x_test[post_id] = body\n",
    "                    y_test[post_id] = category_onehot\n",
    "            \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def tokenize(document):\n",
    "    words = []\n",
    "\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [this.lemmatizer.lemmatize(t.lower()) for t in this.tokenizer.tokenize(sentence)\n",
    "                  if t.lower() not in this.stop_words]\n",
    "        words += tokens\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def tokenize_docs(document):\n",
    "    tokenized_docs = []\n",
    "    this.number_of_documents = len(document)\n",
    "\n",
    "    for key in document.keys():\n",
    "        tokenized_docs.append(tokenize(document[key]))\n",
    "\n",
    "    return tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('reading ', './reuters21578/reut2-000.sgm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulmakhijani/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /Users/rahulmakhijani/anaconda2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('reading ', './reuters21578/reut2-001.sgm')\n",
      "('reading ', './reuters21578/reut2-002.sgm')\n",
      "('reading ', './reuters21578/reut2-003.sgm')\n",
      "('reading ', './reuters21578/reut2-004.sgm')\n",
      "('reading ', './reuters21578/reut2-005.sgm')\n",
      "('reading ', './reuters21578/reut2-006.sgm')\n",
      "('reading ', './reuters21578/reut2-007.sgm')\n",
      "('reading ', './reuters21578/reut2-008.sgm')\n",
      "('reading ', './reuters21578/reut2-009.sgm')\n",
      "('reading ', './reuters21578/reut2-010.sgm')\n",
      "('reading ', './reuters21578/reut2-011.sgm')\n",
      "('reading ', './reuters21578/reut2-012.sgm')\n",
      "('reading ', './reuters21578/reut2-013.sgm')\n",
      "('reading ', './reuters21578/reut2-014.sgm')\n",
      "('reading ', './reuters21578/reut2-015.sgm')\n",
      "('reading ', './reuters21578/reut2-016.sgm')\n",
      "('reading ', './reuters21578/reut2-017.sgm')\n",
      "('reading ', './reuters21578/reut2-018.sgm')\n",
      "('reading ', './reuters21578/reut2-019.sgm')\n",
      "('reading ', './reuters21578/reut2-020.sgm')\n",
      "('reading ', './reuters21578/reut2-021.sgm')\n"
     ]
    }
   ],
   "source": [
    "generate_categories()\n",
    "(x_train, y_train), (x_test, y_test) = read_retuters_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rahulmakhijani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    " # wordnet\n",
    "num_features = 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulmakhijani/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/Users/rahulmakhijani/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:40: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "x_train_token = tokenize_docs(x_train)\n",
    "\n",
    "w2v_model = Word2Vec(x_train_token,\n",
    "                     size=num_features,\n",
    "                     min_count=1,\n",
    "                     window=10)\n",
    "w2v_model.init_sims(replace=True)\n",
    "\n",
    "x_train = vectorize_docs(x_train_token, w2v_model)\n",
    "y_train = vectorize_categories(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulmakhijani/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/Users/rahulmakhijani/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:40: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "x_test_token = tokenize_docs(x_test)\n",
    "\n",
    "w2v_model = Word2Vec(x_test_token,\n",
    "                         size=num_features,\n",
    "                         min_count=1,\n",
    "                         window=10)\n",
    "\n",
    "w2v_model.init_sims(replace=True)\n",
    "x_test = vectorize_docs(x_test_token, w2v_model)\n",
    "y_test = vectorize_categories(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(14668, 100, 500)\n",
      "(14668, 135)\n",
      "(6910, 100, 500)\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, [14668, 50000])\n",
    "x_test = np.reshape(x_test, [6910, 50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14668, 50000)\n",
      "(14668, 135)\n",
      "(6910, 50000)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, training cost = 338.885\n",
      "Iteration: 1000, training cost = 92.091\n",
      "Iteration: 2000, training cost = 89.2283\n",
      "Iteration: 3000, training cost = 52.4993\n",
      "Iteration: 4000, training cost = 44.3524\n",
      "Iteration: 5000, training cost = 30.8331\n",
      "Iteration: 6000, training cost = 48.5827\n",
      "Iteration: 7000, training cost = 34.5085\n",
      "Iteration: 8000, training cost = 16.4638\n",
      "Iteration: 9000, training cost = 22.0427\n",
      "Iteration: 10000, training cost = 23.1173\n",
      "Iteration: 11000, training cost = 33.3611\n",
      "Iteration: 12000, training cost = 13.0437\n",
      "Iteration: 13000, training cost = 29.668\n",
      "Iteration: 14000, training cost = 19.0635\n",
      "Iteration: 15000, training cost = 6.15959\n",
      "Iteration: 16000, training cost = 25.9551\n",
      "Iteration: 17000, training cost = 40.7195\n",
      "Iteration: 18000, training cost = 43.2576\n",
      "Iteration: 19000, training cost = 19.8739\n",
      "Train accuracy = 0.661372, Train cost = 3368.88\n",
      "Test accuracy = 0.531983, Test cost = 13020.6\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 50000])\n",
    "Y = tf.placeholder(tf.float32, [None, 135 ])\n",
    "\n",
    "# Define parameters W and b of your model\n",
    "W1 = tf.get_variable(\"W11\", shape=[50000, 50], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.zeros([50]))\n",
    "W2 = tf.get_variable(\"W12\", shape=[50, 135], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.zeros([135]))\n",
    "\n",
    "# Define your model's tensorflow graph\n",
    "Z1 = tf.matmul(X,W1) + b1\n",
    "A1 = tf.nn.relu(Z1)\n",
    "Z2 = tf.matmul(A1,W2) + b2\n",
    "A2 = tf.nn.softmax(Z2)\n",
    "\n",
    "# Compute the cost function\n",
    "cross_entropy_cost = -tf.reduce_sum(Y * tf.log(A2))\n",
    "\n",
    "# Define accuracy metric\n",
    "num_correct = tf.equal(tf.argmax(A2,1), tf.argmax(Y,1))\n",
    "num_correct = tf.cast(num_correct, tf.float32)\n",
    "accuracy = tf.reduce_mean(num_correct)\n",
    "\n",
    "# Define optimization method, learning rate and the the training step\n",
    "optimizer = tf.train.AdamOptimizer(0.00005)\n",
    "train_step = optimizer.minimize(cross_entropy_cost)\n",
    "\n",
    "# Initialize the variables of the graph, create tensorflow session and run the initialization of global variables.\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Implement the Optimization Loop for 100 iterations\n",
    "for i in range(20000):\n",
    "    # Load batch of images and labels\n",
    "    batch_X, batch_Y = next_batch(100, x_train, y_train)\n",
    "    # Create feed dictionary\n",
    "    feed_dict={X: batch_X, Y: batch_Y}\n",
    "    # Run the session train\n",
    "    _, cost = sess.run([train_step, cross_entropy_cost], feed_dict=feed_dict)\n",
    "    # Print cost and iteration\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Iteration: \" + str(i) + \", training cost = \" + str(cost))\n",
    "\n",
    "# Evaluate your accuracy and cost on the train and test sets\n",
    "train_data={X: x_train, Y: y_train}\n",
    "a,c = sess.run([accuracy, cross_entropy_cost], feed_dict=train_data)\n",
    "print(\"Train accuracy = \" + str(a) + \", Train cost = \" + str(c))\n",
    "\n",
    "test_data={X: x_test, Y: y_test}\n",
    "a,c = sess.run([accuracy, cross_entropy_cost], feed_dict=test_data)\n",
    "print(\"Test accuracy = \" + str(a) + \", Test cost = \" + str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "m = 14668\n",
    "seed = 1 \n",
    "minibatch_size = 64\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.005).minimize(cross_entropy_cost)\n",
    "costs = []   \n",
    "\n",
    "# Initialize all the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start the session to compute the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    # Run the initialization\n",
    "    sess.run(init)\n",
    "        \n",
    "    # Do the training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "        # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "            _ , minibatch_cost = sess.run([optimizer, cross_entropy_cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "            epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "        # Define accuracy metric\n",
    "    num_correct = tf.equal(tf.argmax(A2,1), tf.argmax(Y,1))\n",
    "    num_correct = tf.cast(num_correct, tf.float32)\n",
    "    accuracy = tf.reduce_mean(num_correct)\n",
    "\n",
    "    print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "    print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM  code \n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "import sys\n",
    "\n",
    "this = sys.modules[__name__]\n",
    "\n",
    "\n",
    "def lstm(X_train, Y_train, X_test, Y_test):\n",
    "    \"\"\"Create the LSTM model.\"\"\"\n",
    "    document_max_num_words = 100\n",
    "    num_features = 500\n",
    "    num_categories = 135\n",
    "\n",
    "    tb_callback = keras.callbacks.TensorBoard(log_dir='./tb', histogram_freq=0,\n",
    "                                              write_graph=True, write_images=True)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(int(document_max_num_words * 1.5), input_shape=(document_max_num_words, num_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_categories))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, Y_train, batch_size=128, nb_epoch=5,\n",
    "              validation_data=(X_test, Y_test), callbacks=[tb_callback])\n",
    "\n",
    "    model.save('lstm_reuters.h5')\n",
    "\n",
    "    score, acc = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "\n",
    "    print('Score: %1.4f' % score)\n",
    "    print('Accuracy: %1.4f' % acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
